{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_from_scratch_GPU.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUF5I_cBBEB-"
      },
      "source": [
        "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "#EXERCICE : ACCELERATION GPU D'UN CNN (basique) CODE EN PYTHON/NUMPY\n",
        "#\n",
        "# -> Le but de cet exercice est\n",
        "#    (1) de bien comprendre comment les parametres d'un reseau de neurones sont appris\n",
        "#     et en particulier comment fonctionne la backpropagation. L'implementation  python\n",
        "#    de differentes couches de CNN est donnee. Le CNN apprend alors a reconnaitre des 0\n",
        "#    et des 1 du jeu de donnees MNIST.\n",
        "#    (2) rendre le code le plus rapide possible en utilisant de l'acceleration GPU avec\n",
        "#    openCL ou bien CUDA.\n",
        "#\n",
        "#-> Une fois le code compris ... un seul objectif : l'accelerer avec du code GPU !\n",
        "#\n",
        "#-> Conseil : Si le temps est limite, focalisez vous sur l'amélioration de\n",
        "#             'conv_backward', avec au debut un code openCL ou CUDA qui recode\n",
        "#             l'algorithme sequentiel puis ensuite sa parallelisation. Il sera alors\n",
        "#             evident de mesurer l'impact de la parallelisation !\n",
        "#             Une alternative est de se focaliser sur 'dense_forward' qui permet\n",
        "#             de comparer du code bien optimise avec numpy contre du code GPU. Cette\n",
        "#             alternative est plus simple a coder mais les gains potentiels sont\n",
        "#             moindres.\n",
        "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9P-D-ABpBJFY"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRPCWta-BLlP"
      },
      "source": [
        "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "#                     convolutional layer\n",
        "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "def init_convolution_filter(size):\n",
        "  \"\"\"\n",
        "  size must be a list of the shape [size_h,size_w]\n",
        "  \"\"\"\n",
        "  w=(0.5+np.random.randn(size[0],size[1]))/(size[0]*size[1])\n",
        "  return w\n",
        "\n",
        "\n",
        "def conv_forward(input, w):\n",
        "    \"\"\"\n",
        "    INSPIRED BY: https://gist.github.com/neodelphis\n",
        "    Remark: here stride=1 / no padding / image has only one layer / only one filter\n",
        "\n",
        "    A naive implementation of the forward pass for a convolutional layer.\n",
        "    The input consists of N observations, each of height H and\n",
        "    width W.\n",
        "    We convolve each input with a filter of height HH and width WW.\n",
        "    Input:\n",
        "    - input: Input data of shape (N, H, W)\n",
        "    - w: Filter weights of shape (HH, WW)\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - output: Output data, of shape (N, H', W') where H' and W' are given by H'=H and W'=W\n",
        "    - cache: (input, w)\n",
        "    \"\"\"\n",
        "\n",
        "    N, H, W = input.shape\n",
        "    HH, WW = w.shape\n",
        "\n",
        "    # dimensions de la sortie (tests sur la validité des choix necessaires ensuite / sinon padding)\n",
        "    H_ = H\n",
        "    W_ = W\n",
        "\n",
        "    output = np.zeros((N, H_, W_))\n",
        "\n",
        "    # Version sans vectorisation\n",
        "    for n in range(N):       # On parcourt toutes les images\n",
        "            for i in range(H_): # indices du résultat\n",
        "                for j in range(W_):\n",
        "                    for k in range(HH): # indices du filtre\n",
        "                        for l in range(WW):\n",
        "                          if i+k<H and j+l<W:   #test whether we're inside the image\n",
        "                                output[n,i,j] += input[n, i+k, j+l] * w[k, l]\n",
        "\n",
        "    cache = (input, w)\n",
        "    return output, cache\n",
        "\n",
        "\n",
        "\n",
        "def conv_backward(grad_output, cache):\n",
        "    \"\"\"\n",
        "    INSPIRED BY: https://gist.github.com/neodelphis\n",
        "    Remark: here stride=1 / no padding / image has only one layer / only one filter\n",
        "\n",
        "    A naive implementation of the backward pass for a convolutional layer.\n",
        "\n",
        "    Inputs:\n",
        "    - grad_output: Upstream derivatives. -> Gradient of the loss at the output of the layer\n",
        "    - cache: A tuple of (input, w) as in conv_forward_naive\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - grad_input: Gradient of the loss at the input of the layer wrt input\n",
        "    - grad_w: Gradient of the loss at the input of the layer wrt w\n",
        "    \"\"\"\n",
        "\n",
        "    # Récupération des variables\n",
        "    input, w = cache\n",
        "\n",
        "    # Initialisations\n",
        "    grad_input = np.zeros_like(input)\n",
        "    grad_w = np.zeros_like(w)\n",
        "\n",
        "    # Dimensions\n",
        "    N, H, W = input.shape\n",
        "    HH, WW = w.shape\n",
        "    _, H_, W_ = grad_output.shape   #H_ and W_ sould be equal to H and W\n",
        "\n",
        "    # Version sans vectorisation\n",
        "    for n in range(N):       # On parcourt toutes les images\n",
        "            for i in range(HH): # indices du résultat\n",
        "                for j in range(WW):\n",
        "                    for k in range(H_): # indices du filtre\n",
        "                        for l in range(W_):\n",
        "                          if i+k<H_ and j+l<W_:\n",
        "                                grad_w[i,j] += input[n, i+k, j+l] * grad_output[n, k, l]\n",
        "\n",
        "\n",
        "    # Version sans vectorisation\n",
        "    for n in range(N):       # On parcourt toutes les images\n",
        "            for i in range(H): # indices de l'entrée participant au résultat\n",
        "                for j in range(W):\n",
        "                    for k in range(HH): # indices du filtre\n",
        "                        for l in range(WW):\n",
        "                          if i+k<H and j+l<W:\n",
        "                                grad_input[n,i,j] += grad_output[n, i+k, j+l] * w[HH-k-1,WW-l-1]\n",
        "\n",
        "    return grad_input, grad_w"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dwUcIolBTBh"
      },
      "source": [
        "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "#                     dense layer\n",
        "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "def init_dense_layer_weights(input_size, output_size):\n",
        "  \"\"\"\n",
        "  INSPIRED BY:  https://towardsdatascience.com/building-neural-network-from-scratch-9c88535bf8e9\n",
        "\n",
        "  initialize the weights of a dense layer\n",
        "  \"\"\"\n",
        "  weights = np.random.normal(loc=0.0,scale = np.sqrt(2/(input_size+output_size)),\n",
        "                                        size = (input_size,output_size))\n",
        "  return weights\n",
        "\n",
        "def dense_forward(input,weights):\n",
        "  \"\"\"\n",
        "  INSPIRED BY:  https://towardsdatascience.com/building-neural-network-from-scratch-9c88535bf8e9\n",
        "\n",
        "  forward phase of a dense layer\n",
        "  \"\"\"\n",
        "  cache = (input,weights)\n",
        "  output = np.dot(input,weights)\n",
        "\n",
        "  return output, cache\n",
        "\n",
        "def dense_backward(grad_output,cache):\n",
        "  \"\"\"\n",
        "  INSPIRED BY:  https://towardsdatascience.com/building-neural-network-from-scratch-9c88535bf8e9\n",
        "\n",
        "  backward phase of a dense layer. The cache was returned by dense_forward.\n",
        "  \"\"\"\n",
        "  input,weights = cache\n",
        "\n",
        "  grad_input = np.dot(grad_output, weights.T)\n",
        "\n",
        "  grad_weights = np.dot(input.T, grad_output)\n",
        "\n",
        "  return grad_input , grad_weights\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdtP6nMZBVkL"
      },
      "source": [
        "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "#                     ReLU layer\n",
        "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "def ReLU_forward(input):\n",
        "  \"\"\"\n",
        "  INSPIRED BY:  https://towardsdatascience.com/building-neural-network-from-scratch-9c88535bf8e9\n",
        "  \"\"\"\n",
        "\n",
        "  cache = (input)\n",
        "  output = np.maximum(0,input)\n",
        "  return output , cache\n",
        "\n",
        "def ReLU_backward(grad_output,cache):\n",
        "  \"\"\"\n",
        "  INSPIRED BY:  https://towardsdatascience.com/building-neural-network-from-scratch-9c88535bf8e9\n",
        "  \"\"\"\n",
        "\n",
        "  input = cache\n",
        "  relu_grad = input > 0\n",
        "  grad_input=grad_output*relu_grad\n",
        "\n",
        "  return grad_input"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4ENmxJ_BYlb"
      },
      "source": [
        "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "#                     logistic function\n",
        "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "def Logistic_function(input):\n",
        "  return 1 / (1 + np.exp(-input))\n",
        "\n",
        "def derivative_Logistic_function(input):\n",
        "  lf_input=Logistic_function(input)\n",
        "  return lf_input*(1-lf_input)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1TfA-1xBbEE"
      },
      "source": [
        "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "#                     MAIN\n",
        "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "#1) INIT\n",
        "\n",
        "#get and treat data\n",
        "data=np.genfromtxt('./mnist_0_1.csv',delimiter=',')\n",
        "\n",
        "n_tot=data.shape[0]\n",
        "p=data.shape[1]\n",
        "\n",
        "y_train=data[:int(2.*n_tot/3.),0].reshape(-1,1)\n",
        "X_train=(data[:int(2.*n_tot/3.),1:]/(255.*p)).reshape(-1,28,28)\n",
        "\n",
        "y_test=data[int(2.*n_tot/3.):,0].reshape(-1,1)\n",
        "X_test=(data[int(2.*n_tot/3.):,1:]/(255.*p)).reshape(-1,28,28)\n",
        "\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def ShowMNISTObservation(X_data,y_data,obsNb=0):\n",
        "  plt.clf()\n",
        "  plt.imshow(X_data[obsNb,:].reshape((28,28)))\n",
        "  plt.title('Observation '+str(obsNb)+': Label '+str((y_data[obsNb,0])))\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "#ShowMNISTObservation(X_train,y_train,0)\n",
        "#ShowMNISTObservation(X_train,y_train,1)\n",
        "#...\n",
        "#ShowMNISTObservation(X_test,y_train,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYnm9IrGBfOt"
      },
      "source": [
        "#2) TRAINING\n",
        "#generate the network parameters\n",
        "\n",
        "w_conv=init_convolution_filter([5,5])\n",
        "w_dense=init_dense_layer_weights(28*28, 1)\n",
        "\n",
        "w_conv_init=w_conv.copy()\n",
        "w_dense_init=w_dense.copy()\n",
        "\n",
        "#Stochastic gradient Descent\n",
        "Batch_size=100\n",
        "nb_epochs=2\n",
        "\n",
        "n=X_train.shape[0]\n",
        "\n",
        "total_time_in_conv_backward=0.\n",
        "start_time= time()\n",
        "\n",
        "for epoch in range(nb_epochs):\n",
        "    print('epoch:',epoch)\n",
        "    obsIDs=np.arange(n)\n",
        "    np.random.shuffle(obsIDs)\n",
        "    batch_start=0\n",
        "\n",
        "    while batch_start+Batch_size<n:\n",
        "      x_batch=X_train[obsIDs[batch_start:batch_start+Batch_size],:,:]\n",
        "      y_true_batch=y_train[obsIDs[batch_start:batch_start+Batch_size],:]\n",
        "\n",
        "      #forward phase\n",
        "      output_1 , cache_1 = conv_forward(x_batch, w_conv)\n",
        "      output_2 , cache_2 = ReLU_forward(output_1)\n",
        "      output_3 , cache_3 = dense_forward(output_2.reshape(-1,28*28),w_dense) #the reshape is used to flatten the image\n",
        "      y_pred = Logistic_function(output_3)\n",
        "\n",
        "      MSE_loss = np.mean( np.power(y_pred-y_true_batch,2.) ) #log-likelihood would be slightly more general\n",
        "\n",
        "      #backward phase\n",
        "      grad_MSE_loss = 2*(y_pred-y_true_batch)\n",
        "      grad_output_3=grad_MSE_loss*derivative_Logistic_function(output_3)\n",
        "      grad_output_2 , grad_w_dense = dense_backward(grad_output_3,cache_3)\n",
        "      grad_output_1=ReLU_backward(grad_output_2.reshape(-1,28,28),cache_2)  #the reshape reverts the flattening\n",
        "      rtime = time()\n",
        "      grad_input, grad_w_conv=conv_backward(grad_output_1, cache_1)\n",
        "      rtime = time() - rtime\n",
        "      total_time_in_conv_backward+=rtime\n",
        "\n",
        "      #gradient descent-update\n",
        "      w_dense-=500*grad_w_dense\n",
        "      w_conv-=0.001*grad_w_conv    #REMARK: the learning rate has to be much smaller on the convolutional layer than on the dense layer (the data should be re-scaled in this layer to avoid this)\n",
        "\n",
        "      #prepare the next mini-batch\n",
        "      batch_start+=Batch_size\n",
        "\n",
        "      print(MSE_loss)\n",
        "\n",
        "print('Total time:',time() - start_time)\n",
        "print('Time in conv_backward:',total_time_in_conv_backward)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJxYzadgBh27"
      },
      "source": [
        "#3) TEST\n",
        "\n",
        "#show the trained information\n",
        "\n",
        "plt.imshow(w_dense_init.reshape((28,28)))\n",
        "plt.title('initial dense layer')\n",
        "plt.show()\n",
        "\n",
        "plt.imshow(w_dense.reshape((28,28)))\n",
        "plt.title('Trained dense layer')\n",
        "plt.show()\n",
        "\n",
        "print('Initial convolution filter:\\n',w_conv_init)\n",
        "print('Trained convolution filter:\\n',w_conv)\n",
        "\n",
        "\n",
        "#predictions on test data\n",
        "\n",
        "output_1 , cache_1 = conv_forward(X_test, w_conv)\n",
        "output_2 , cache_2 = ReLU_forward(output_1)\n",
        "output_3 , cache_3 = dense_forward(output_2.reshape(-1,28*28),w_dense)\n",
        "y_pred = Logistic_function(output_3)\n",
        "\n",
        "MSE_loss = np.mean( np.power(y_pred-y_test,2.) )\n",
        "\n",
        "Nb_false_pred=np.sum(np.abs(1*(y_pred>0.5)-y_test))\n",
        "\n",
        "prct_false_pred=100.*Nb_false_pred/y_test.shape[0]\n",
        "\n",
        "print('Percentage of good predictions:',100-prct_false_pred)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}